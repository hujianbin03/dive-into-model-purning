### 1. CIFAR10数据集
**CIFAR10**是一个广泛使用的图像分类数据集，由10个类别的60000张32*32彩色图像组成，每个类别6000张图像，
其中50000张用于训练集，10000张用于测试集。  

CIFAR10数据集的类别包括：airplane(飞机)、automobile(汽车)、bird(鸟)、cat(猫)、deer(鹿)、dog(狗)、
frog(青蛙)、horse(马)、ship(船)和truck(卡车)。

### 2. VGG网络搭建
VGGNet是由牛津大学计算机视觉组于2014年提出的一个深度卷积神经网络。VGGNet的特点是采用了非常小的3*3卷积核，
使用多个小卷积核来替代大的卷积核，增加网络的深度和非线性表达能力。

VGGNet的网络结构非常简单，它包含了若干个卷积层和全连接层，其中卷积层包含了多个卷积核，每个卷积核的大小都是3x3。
网络的最后一层是全连接层，用于将卷积层的输出映射到类别标签上。VGGNet的网络结构中，每个卷积层都采用了相同的结构，
即两个3x3的卷积核，每个卷积核后面都跟了一个ReLU激活函数，最后是一个2x2的最大池化层。这个基本单元被称为VGG块，
网络中的所有卷积层都由多个VGG块组成。

### 3. Batch Normalize
[论文参考：Batch Normalization- Accelerating Deep Network Training b y Reducing Internal Covariate Shift](https://readpaper.com/paper/2949117887)

**Batch Normalize**(批标准化)，是一种深度神经网络中常用的正则化方法，旨在缓解深度神经网络中梯度消失或梯度爆炸的问题，
加速训练过程并提高模型的性能。

**Batch Normalize**在训练过程中，对每个mini batch的输出进行标准化，即对每个特征在batch维度上进行标准化，使得输出的
均值和方差分别为0和1。这样做的好处在于，使得每个层的输入都是以相同的方式进行标准化，从而加速了训练过程。

具体来说，**Batch Normalize**可以分为以下几个步骤：
* 对于输入特征x，计算其均值μ 和标准差 σ。
* 标准化：将特征x标准化为 x ^ = (x − μ) / Math.sqrt(σ ^ 2 +  ϵ), 其中 ϵ \epsilonϵ 是一个小的常数，防止除数为 0。
* 对标准化后的特征进行缩放和平移：BN(X) = gammaγ * x ^ + β，其中 gammaγ 和 β 是可学习的参数，使得模型可以自适应地选择适当的
缩放和平移，从而提高模型的拟合能力。
* 对于每个mini batch，通过梯度下降来更新gammaγ 和 β

### 4.L1&L2正则
我们说的正则化，就是在原来的Loss Function的基础上，**加上了一些正则化项或者称为模型复杂度惩罚项**
* 在剪枝中，L1正则化会用在Batch Normalization上面的gammaγ,实现稀疏训练。

**为什么使用L1正则化可以实现稀疏训练？**
L1正则化是一种对模型权重进行惩罚的方法，它将权重中的小值变为0，从而实现模型的稀疏化。在Batch Normalization中，gammaγ
是缩放因子，它用来缩放归一化的输出，而且gammaγ的初始值通常被设置为1，如果对其进行L1正则化惩罚，会使得模型更倾向于将一些
通道的权重设置为0，从而实现通道的剪枝，减少模型参数量和计算量。因此，在剪枝中，L1正则化被广泛应用于Batch Normalization
的缩放因子gammaγ上.

**为什么使用L1正则化，不使用L2正则化？**
L1正则化在想要寻找一个能够大幅减少权值数量的最优解时很多用。L1正则化对权值施加的惩罚不像L2正则化那样平滑，它倾向于让一些权
值变为0.对于gammaγ系数，因为它们的值用于调节每个通道的缩放因子，使其接近于1，而一些通道的重要性可能不如其他通道。因此，使用
L1正则化有助于找到仅仅使用少量通道可以获得相同性能的gammaγ系数。而L2正则化倾向于使得所有的gammaγ系数都很小，但非零。因此
在使用L1正则化时，可以通过稀疏化权重获得一些模型压缩和加速的好处。

### 5.train


**问题**: 深度学习模型里面的卷积层出来之后的特征有非常多，这里面会不会存在一些没有价值的特征及其相关的连接？
如何判断一个特征及其连接是否有价值？

A：在Batch Normalize层的缩放因子上施加L1正则化(这是上面这篇论文的核心思想，更多细节请自行阅读论文😂)
优点  
* 不需要对现有的CNN架构进行任何更改
* 使用L1正则化将BN缩放因子的值推向0
  * 使我们能够识别不重要的通道或者神经元，因为每个缩放因子对应特定的卷积通道或者全连接层的神经元
  * 这有利于接下来的步骤进行通道剪枝
* 附加的正则化项很少会损害性能，不仅如此，在某些情况下，它会导致更高的泛化精度
* 剪枝不重要的通道有时会暂时降低性能，但这个效应可以通过接下来的修剪网络的微调来弥补
* 剪枝后，由此得到的较窄的网络在模型大小、运行时内存和计算操作方面比初始的宽网络更加紧凑。上述过程重复几次，得到
一个多通道网络瘦身的方案，从而实现更加紧凑的网络。

**问题**: 稀疏(约束)训练的必要性？
约束训练可以使得模型更易于剪枝。在约束训练中，模型会学习到一些通道或者权重系数比较不重要的信息。而这些信息在剪枝
过着中得到应用，从而达到模型压缩的效果。而如果直接进行剪枝操作，可能会出现一些问题，比如剪枝后的模型精度大幅下降、
剪枝不均匀等。因此，在剪枝操作之前，通过稀疏训练的返方式，可以更好的准确的确定哪些通道和权重系数可以被剪掉，从而
避免上述问题的发生。








































