### 1. 非结构化剪枝   
    >非结构化剪枝：是指不按照某种固定的结构对神经网络进行裁剪，而是根据某些规则需要进行裁剪。

   #### 1. 细粒度剪枝 fine-grained
        细粒度剪枝：是指针对神经玩啰的每个权重进行剪枝，相对于结构剪枝而言，细粒度剪枝不会改变神经网络的结构。
        细粒度剪枝通过移除不重要的权重，可以达到减小神经网络模型大小的目的，从而提高模型的运行速度和存储效率。  
      **实现：**  
        在细粒度剪枝中，我们可以按比例裁剪卷积层的权重。具体来说，我们可以获取卷积核权重张量的数据，然后计算
        剪枝的权重数量，找到需要保留的权重的最小阀值，将小于阀值的权重置为0，并将剪枝后的权重张量赋给卷积层的
        权重。  
        **注意：剪枝中，一般不会修改偏置bias,因为偏置项数量较少，对整个模型的性能影响不大。**
   #### 2. 向量剪枝 vector-level
        向量剪枝是一种非结构化剪枝方法，它是将某些列和行上的参数设置为0，从而减少参数。  
        **向量剪枝仅仅是将某些权重设置为0，可能会导致权重矩阵的稀疏性，且不一定带来较大的压缩效果。**
   #### 3. 卷积核剪枝 kernel-level   
        是指对神经网络中的卷积核进行剪枝。一般来说我们可以通过计算每个filter的L2范数，根据L2范数排序，选择
        剪枝比例最高的一定数量的filter.
      > 范数：是向量空间的一种函数，其用来衡量向量的大小。在数学中，范数是一种将向量映射到非负实数的函数。  
        * 范数有很多种，例如L1、L2范数等。其中L2范数也被称为欧几里得范数，它是指向量各元素的的平方和的平方根。      
        * L2范数在机器学习和深度学习中经常被用来作为模型的正则化项，以控制模型的复杂度，防止过拟合。  
        * L1范数也是常用的范数之一，它是指向量各元素的绝对值之和。相比于L2范数，L1范数可以使向量更加稀疏，适用于稀疏性较强的场景。
   
   **卷积核输入、输出层与卷积核的数量关系：**  
   卷积核通道数 = 卷积输入层的通道数
   卷积核的个数 = 卷积输出层通道数(深度)
   **卷积核kernel和滤波器filter的区别：**  
   * 在输入input只有一个通道的时候，卷积核就相当于滤波器  
   * 但在一般情况下，大多数输入图像都是RGB 3通道，它们就是两个完全不同的概念。每个滤波器恰好是卷积核的一个集合。
     * 卷积核是由长、宽来指定的，是一个二维的概念。滤波器是由长、宽和深度来指定的，是一个三维的概念。
     * 滤波器可以看作是卷积核的集合。比卷积核多了一个维度：深度，即输出的通道数，也是卷积核的个数。
### 2. 结构化剪枝
    >结构化剪枝：是一种卷积层结构进行剪枝的方法。它通过对不同维度上的元素进行聚合，以便在不破坏卷积层结构的情况下
     实现剪枝。

   #### 1. 滤波器剪枝
   #### 2. 通道剪枝
   #### 3. 层剪枝
   
### 3. 非结构化剪枝&结构化剪枝  
    区别：  
    * **颗粒度不同:**  
     非结构化剪枝主要是通过对权重矩阵中的单个或整行、整列的权重进行修剪。结构化剪枝的基本修剪单元是卷积核、滤波器或者通道。
    * **修剪的影响不同:**  
    非结构化剪枝会使得权重矩阵变成稀疏矩阵，需要硬件和软件来支持搞笑的稀疏矩阵计算，否则对性能是没有提升的。结构化剪枝没有改变
   权重矩阵本身的稀疏程度。
    * **针对性不同:**  
    重点是结构二字，非结构化是指没有按照某个结构进行剪枝，而是按照某种规则(如大小、随机概率等)，而结构化刚好相反，它是按照某
    种结构进行剪枝的，如kernel、filter、channel.非结构化剪枝就是有点无脑，不考虑结构，直接操作所有元素，结构化剪枝就是考虑
    数据之间的相关性(结构性)进行剪枝。
### 4. 剪枝标准
    剪枝标准pruning criterion是指剪枝时所采用的判断依据，也是剪枝的关键。三种常见的剪枝标准有：基于权重大小的剪枝标准、
    基于梯度幅度修剪、基于梯度和权重大小的混合标准。  
    值得注意的是，这三种方法的最终剪枝对象都是权重，也就是卷积核权重。它们的不同之处在于，所依据的标准不同。例如基于权重
    大小的剪枝标准是一句权重绝对值大小来进行修剪的。而基于梯度幅度是根据梯度的绝对值大小来进行修剪的。因此，这些方法都是
    对卷积核权重进行修剪，只是针对不同的标准进行修剪。
   #### 1. 基于权重大小的剪枝标准
基于权重大小的剪枝标准weight-based pruning criterion，即根据权重大小来决定哪些权重需要剪枝。权重越小，越容易被剪枝。
前面提到的非结构化剪枝都是基于卷积核的权重进行剪枝的，也就是基于权重大小的剪枝标准。

   #### 2. 基于梯度幅度修剪
基于梯度幅度剪枝标准gradient-based pruning criterion，即根据权重的梯度幅度来决定哪些权重需要剪枝。梯度幅度越小的权重
越容易被剪枝。
**为什么不能仅依靠梯度幅度进行修剪**
A: 如果只依靠梯度大小进行裁剪可能会导致丢失某些有用的信息，因为一些梯度较小但仍然重要的参数可能被错误的修剪掉。另一方面，仅仅
使用梯度幅度进行修剪还会导致修剪后的模型结构变得更加稀疏，使得模型的性能和鲁棒性下降。因此，通常需要结合权重大小等其他因素
一起考虑，综合多种因素进行修剪。

   #### 3. 基于梯度和权重大小的混合标准
基于梯度和权重大小的混合剪枝mixed pruning criterion，即根据权重大小和梯度幅度综合考虑来决定哪些权重需要剪枝。综合权重大小
和梯度幅度，可以更好的选择需要剪枝的权重，提高剪枝效果。


### 5. 总结
模型剪枝的方法，主要可以分为结构化和非结构化剪枝，其中结构化剪枝又可以按照不同的结构，如kernel、filter、channel进行剪枝。
剪枝标准，项非结构化剪枝和结构剪枝都是针对卷积核进行剪枝的，即基于权重带下的剪枝标准，除此之外，还有基于梯度幅度剪枝以及
混合标准。
   



















